<h1 align="center">ğŸ” â¡ï¸ğŸ’» Text-to-Code Generation using Deep Learning</h1>

<p align="center">
  <i>Transform natural language instructions into executable Python code using NLP & Large Language Models</i>
</p>

---

<h2>ğŸ“Œ Overview</h2>

<p>
  This project leverages <strong>GPT-Neo 2.7B</strong> to automatically generate Python code from text-based instructions. 
  By fine-tuning the model with the <strong>iamtarun/python_code_instructions_18k_alpaca dataset</strong>, we enable precise 
  and contextual code generation.
</p>

<p>âœ¨ **Key Features:**</p>
<ul>
  <li>ğŸ”¹ Fine-tuned <strong>GPT-Neo 2.7B</strong> for optimized code generation</li>
  <li>ğŸ”¹ Utilizes <strong>LoRA (Low-Rank Adaptation)</strong> for efficient fine-tuning</li>
  <li>ğŸ”¹ <strong>Model Quantization</strong> for reduced computational requirements</li>
  <li>ğŸ”¹ <strong>Web Application</strong> for real-time code generation</li>
  <li>ğŸ”¹ Trained on a dataset of <strong>18,000+ text-code pairs</strong></li>
</ul>

ğŸ“„ **For more details**, check the <a href="https://github.com/aryansharma7341/Text-to-Code-Generation/tree/main/Documentation">Documentation</a>.

---

<h2>ğŸ› ï¸ Project Structure</h2>

<pre>
ğŸ“‚ Text-to-Code-Generation
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ dataset_processing
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ dataset_statistics.py
â”‚   â”œâ”€â”€ tokenizer_setup.py
â”‚
â”œâ”€â”€ ğŸ—ï¸ model_training
â”‚   â”œâ”€â”€ train_gpt_neo.py
â”‚   â”œâ”€â”€ fine_tune_LoRA.py
â”‚   â”œâ”€â”€ model_quantization.py
â”‚   â””â”€â”€ evaluation.py
â”‚
â”œâ”€â”€ ğŸš€ inference
â”‚   â”œâ”€â”€ generate_code.py
â”‚   â”œâ”€â”€ test_cases.py
â”‚   â””â”€â”€ performance_benchmark.py
â”‚
â”œâ”€â”€ ğŸŒ web_application
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ static/
â”‚   â””â”€â”€ api.py
â”‚
â””â”€â”€ ğŸ“– documentation
    â”œâ”€â”€ model_architecture.md
    â”œâ”€â”€ dataset_details.md
    â”œâ”€â”€ optimization_techniques.md
</pre>

---

<h2>ğŸ—ï¸ System Architecture</h2>

<p align="center">
  <img src="https://your-image-link-here.com/architecture.png" alt="System Architecture" width="75%"/>
</p>

---

<h2>ğŸ¥ Demo</h2>

ğŸ“º **Watch the full demo here:** <a href="https://www.youtube.com/watch?v=your-video-link">YouTube Video</a>

<p align="center">
  <img src="https://your-image-link-here.com/demo.gif" alt="Demo GIF" width="75%"/>
</p>

---

<h2>ğŸ“Š Model Performance</h2>

<table>
  <tr>
    <th>Model</th>
    <th>Dataset</th>
    <th>BLEU Score</th>
    <th>Training Time</th>
  </tr>
  <tr>
    <td><b>GPT-Neo 2.7B (LoRA Fine-Tuned)</b></td>
    <td>Python_Code_Instructions_18K</td>
    <td><b>79.4</b></td>
    <td>8 hours</td>
  </tr>
  <tr>
    <td>GPT-3.5 (API-based)</td>
    <td>OpenAI Code Dataset</td>
    <td>82.1</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>CodeT5</td>
    <td>CodeSearchNet</td>
    <td>75.8</td>
    <td>12 hours</td>
  </tr>
</table>

ğŸ”¹ **Fine-tuned GPT-Neo 2.7B achieved an impressive BLEU Score of 79.4**, making it highly effective for text-to-code generation.

---

<h2>âš¡ Installation & Setup</h2>

<h3>ğŸ›  Prerequisites</h3>
<ul>
  <li>âœ… Python 3.8+</li>
  <li>âœ… PyTorch</li>
  <li>âœ… Hugging Face Transformers</li>
  <li>âœ… Flask</li>
  <li>âœ… BitsAndBytes (for quantization)</li>
</ul>

<h3>ğŸš€ Steps</h3>

<pre>
# Clone the repository
git clone https://github.com/aryansharma7341/Text-to-Code-Generation.git
cd Text-to-Code-Generation

# Install required dependencies
pip install -r requirements.txt

# Run the Flask application
cd web_application
python app.py
</pre>

---

<h2>ğŸ† Contributors</h2>
<p>ğŸ‘¤ <strong>Aryan Sharma</strong><br>
ğŸ“§ <a href="aryansharma7341.as@gmail.com">Email</a><br>
ğŸ”— <a href="https://github.com/aryansharma7341">GitHub</a><br>
ğŸ”— <a href="www.linkedin.com/in/aryansharma7341">LinkedIn</a></p>

---

<p align="center">ğŸŒŸ <strong>If you find this project useful, don't forget to â­ the repository!</strong> ğŸš€</p>
